{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "source_path = 'data/letters_source.txt'\n",
    "target_path = 'data/letters_target.txt'\n",
    "\n",
    "with open(source_path, 'r') as f:\n",
    "    source_sentences = f.read()\n",
    "    \n",
    "with open(target_path, 'r') as f:\n",
    "    target_sentences = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bsaqq',\n",
       " 'npy',\n",
       " 'lbwuj',\n",
       " 'bqv',\n",
       " 'kial',\n",
       " 'tddam',\n",
       " 'edxpjpg',\n",
       " 'nspv',\n",
       " 'huloz',\n",
       " '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_sentences[:50].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abqqs',\n",
       " 'npy',\n",
       " 'bjluw',\n",
       " 'bqv',\n",
       " 'aikl',\n",
       " 'addmt',\n",
       " 'degjppx',\n",
       " 'npsv',\n",
       " 'hlouz',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sentences[:50].split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<UNK>', '<GO>', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print( ['<PAD>', '<UNK>', '<GO>', '<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsaqq\n",
      "npy\n",
      "\n",
      "{0: '<PAD>', 1: '<UNK>', 2: '<GO>', 3: '<EOS>', 4: 'e', 5: 'h', 6: 'k', 7: 'v', 8: 'p', 9: 'f', 10: 'c', 11: 't', 12: 'd', 13: 'm', 14: 'g', 15: 'b', 16: 'r', 17: 'y', 18: 's', 19: 'z', 20: 'l', 21: 'q', 22: 'n', 23: 'j', 24: 'i', 25: 'x', 26: 'w', 27: 'u', 28: 'o', 29: 'a'}\n",
      "{'<PAD>': 0, '<UNK>': 1, '<GO>': 2, '<EOS>': 3, 'e': 4, 'h': 5, 'k': 6, 'v': 7, 'p': 8, 'f': 9, 'c': 10, 't': 11, 'd': 12, 'm': 13, 'g': 14, 'b': 15, 'r': 16, 'y': 17, 's': 18, 'z': 19, 'l': 20, 'q': 21, 'n': 22, 'j': 23, 'i': 24, 'x': 25, 'w': 26, 'u': 27, 'o': 28, 'a': 29}\n",
      "Example source sequence\n",
      "[[15, 18, 29, 21, 21], [22, 8, 17], [20, 15, 26, 27, 23]]\n",
      "\n",
      "\n",
      "Example target sequence\n",
      "[[29, 15, 21, 21, 18, 3], [22, 8, 17, 3], [15, 23, 20, 27, 26, 3]]\n"
     ]
    }
   ],
   "source": [
    "def extract_character_vocab(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "\n",
    "    set_words = set([character for line in data.split('\\n') for character in line])\n",
    "    int_to_vocab = {word_i: word for word_i, word in enumerate(special_words + list(set_words))}\n",
    "    vocab_to_int = {word: word_i for word_i, word in int_to_vocab.items()}\n",
    "\n",
    "    return int_to_vocab, vocab_to_int\n",
    "\n",
    "# Build int2letter and letter2int dicts\n",
    "source_int_to_letter, source_letter_to_int = extract_character_vocab(source_sentences)\n",
    "target_int_to_letter, target_letter_to_int = extract_character_vocab(target_sentences)\n",
    "print(source_sentences[:10])\n",
    "print(source_int_to_letter)\n",
    "print(source_letter_to_int)\n",
    "# print(source_letter_to_int)\n",
    "# Convert characters to ids\n",
    "source_letter_ids = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) for letter in line] for line in source_sentences.split('\\n')]\n",
    "target_letter_ids = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) for letter in line] + [target_letter_to_int['<EOS>']] for line in target_sentences.split('\\n')] \n",
    "\n",
    "print(\"Example source sequence\")\n",
    "print(source_letter_ids[:3])\n",
    "print(\"\\n\")\n",
    "print(\"Example target sequence\")\n",
    "print(target_letter_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is the final shape we need them to be in. We can now proceed to building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "#### Check the Version of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 60\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    input_data = tf.placeholder(dtype=tf.int32,\n",
    "                                shape=[None, None],\n",
    "                                name='input')\n",
    "    targets = tf.placeholder(dtype=tf.int32,\n",
    "                             shape=[None, None],\n",
    "                             name='targets')\n",
    "    lr = tf.placeholder(dtype=tf.float32,\n",
    "                        shape=None,\n",
    "                        name='learning_rate')\n",
    "    target_sequence_length = tf.placeholder(tf.int32,\n",
    "                                            shape=[None,],\n",
    "                                            name='target_sequence_length')\n",
    "    \n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_length')\n",
    "    source_sequence_length = tf.placeholder(dtype=tf.int32,\n",
    "                                            shape=[None,],\n",
    "                                            name='source_sequence_length')\n",
    "    \n",
    "    return input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence to Sequence Model\n",
    "\n",
    "We can now start defining the functions that will build the seq2seq model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Encoder\n",
    "\n",
    "The first bit of the model we'll build is the encoder. Here, we'll embed the input data, construct our encoder, then pass the embedded data to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers,\n",
    "                   source_sequence_length, source_vocab_size,\n",
    "                   encoding_embedding_size):\n",
    "    \n",
    "    ###encoder embedding###\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(ids=input_data, vocab_size=source_vocab_size, embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    ###rnn cell###\n",
    "    def get_rnn(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(num_units=rnn_size,\n",
    "                                                initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "        return enc_cell\n",
    "    \n",
    "    ###get rnn###\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([get_rnn(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    ###sequence_length: dealing with different lengths of inputs###\n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs=enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers,\n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "\n",
    "\n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    ###sequence_length: dealing with different lengths of inputs###\n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Decoder\n",
    "\n",
    "#### Process Decoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the input we'll feed to the decoder\n",
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Set up the decoder components\n",
    "\n",
    "##### 1- Embedding\n",
    "Now that we have prepared the inputs to the training decoder, we need to embed them so they can be ready to be passed to the decoder. \n",
    "\n",
    "##### 2- Decoder Cell\n",
    "Then we declare our decoder cell. \n",
    "\n",
    "We need to declare a decoder for the training process, and a decoder for the inference/prediction process. \n",
    "\n",
    "##### 3- Dense output layer\n",
    "Before we move to declaring our decoders, we'll need to create the output layer, which will be a tensorflow.python.layers.core.Dense layer that translates the outputs of the decoder to logits that tell us which element of the decoder vocabulary the decoder is choosing to output at each time step.\n",
    "\n",
    "##### 4- Training decoder\n",
    "Essentially, we'll be creating two decoders which share their parameters. One for training and one for inference. \n",
    "\n",
    "##### 5- Inference decoder\n",
    "The inference decoder is the one we'll use when we deploy our model to the wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layers(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                    target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "    ### Decoder Embedding ###\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    ### Construct the decoder cell ###\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(num_units=rnn_size,\n",
    "                                                initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "    \n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell(cells=[make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    \n",
    "    ### Dense layer ###\n",
    "    out_layer = Dense(units=target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    \n",
    "    \n",
    "    ### Training Decoder ###\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        ### Helper for the training process. Used by BasicDecoder to read inputs###\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        \n",
    "        ### Basic decoder ###\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, \n",
    "                                                           helper=training_helper,\n",
    "                                                           initial_state=enc_state,\n",
    "                                                           output_layer=out_layer)\n",
    "        \n",
    "        ### Performing dynamic decoding using the decoder ###\n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                                    impute_finished=True,\n",
    "                                                                    maximum_iterations=max_target_sequence_length)\n",
    "        \n",
    "    ### Inference Decoder ###\n",
    "    # Reuses the same parameters trained by the training process\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        \n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        \n",
    "        ### Helper for the inference process ###\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=dec_embeddings,\n",
    "                                                                    start_tokens = start_tokens,\n",
    "                                                                    end_token = target_letter_to_int['<EOS>'])\n",
    "        \n",
    "        ### Basic decoder ###\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
    "                                                            helper=inference_helper,\n",
    "                                                            initial_state=enc_state,\n",
    "                                                            output_layer=out_layer)\n",
    "        \n",
    "        ### Inference decoder output###\n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(decoder=inference_decoder,\n",
    "                                                                     impute_finished=True,\n",
    "                                                                     maximum_iterations=max_target_sequence_length)\n",
    "        \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                   target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "    # 1. Decoder Embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    # 2. Construct the decoder cell\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    # 3. Dense layer to translate the decoder's output at each time \n",
    "    # step into a choice from the target vocabulary\n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    # 4. Set up a training decoder and an inference decoder\n",
    "    # Training Decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "\n",
    "        # Helper for the training process. Used by BasicDecoder to read inputs.\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        \n",
    "        # Basic decoder\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           enc_state,\n",
    "                                                           output_layer) \n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                       impute_finished=True,\n",
    "                                                                       maximum_iterations=max_target_sequence_length)[0]\n",
    "    # 5. Inference Decoder\n",
    "    # Reuses the same parameters trained by the training process\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        # Helper for the inference process.\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                target_letter_to_int['<EOS>'])\n",
    "\n",
    "        # Basic decoder\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        enc_state,\n",
    "                                                        output_layer)\n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_target_sequence_length)[0]\n",
    "         \n",
    "\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Seq2seq model \n",
    "Hooking up the encoder and decoder using the methods we just declared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, targets, lr, target_sequence_length,\n",
    "                  max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers):\n",
    "    \n",
    "    ### encoder layer ###\n",
    "    _, enc_state = encoding_layer(input_data,\n",
    "                                  rnn_size,\n",
    "                                  num_layers,\n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size,\n",
    "                                  enc_embedding_size)\n",
    "    \n",
    "    \n",
    "    # Preparing the target sequences which will be fed to training decoder\n",
    "    dec_input = process_decoder_input(target_data=targets, vocab_to_int=target_letter_to_int, batch_size=batch_size)\n",
    "    \n",
    "    # Passing encoder state and decoder inputs to the decoders\n",
    "    training_decoder_output, inference_decoder_output = decoding_layers(dec_input= dec_input,\n",
    "                                                                        decoding_embedding_size=dec_embedding_size,\n",
    "                                                                        enc_state=enc_state,\n",
    "                                                                        max_target_sequence_length=max_target_sequence_length,\n",
    "                                                                        num_layers=num_layers,\n",
    "                                                                        rnn_size=rnn_size,\n",
    "                                                                        target_letter_to_int=target_letter_to_int,\n",
    "                                                                        target_sequence_length=target_sequence_length)\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, \n",
    "                  max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, \n",
    "                  rnn_size, num_layers):\n",
    "    \n",
    "    # Pass the input data through the encoder. We'll ignore the encoder output, but use the state\n",
    "    _, enc_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size, \n",
    "                                  encoding_embedding_size)\n",
    "    \n",
    "    \n",
    "    # Prepare the target sequences we'll feed to the decoder in training mode\n",
    "    dec_input = process_decoder_input(targets, target_letter_to_int, batch_size)\n",
    "    \n",
    "    # Pass encoder state and decoder inputs to the decoders\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(target_letter_to_int, \n",
    "                                                                       decoding_embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       enc_state, \n",
    "                                                                       dec_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Graph part###\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    ###input layer###\n",
    "    with tf.name_scope('input'):\n",
    "        input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_model_inputs()\n",
    "        \n",
    "    ###seq2seq###\n",
    "    \n",
    "\n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data=input_data,\n",
    "                                                                      targets=targets,\n",
    "                                                                      lr=lr,\n",
    "                                                                      target_sequence_length=target_sequence_length,\n",
    "                                                                      max_target_sequence_length=max_target_sequence_length,\n",
    "                                                                      source_sequence_length=source_sequence_length,\n",
    "                                                                      source_vocab_size=len(source_letter_to_int),\n",
    "                                                                      target_vocab_size=len(target_letter_to_int),\n",
    "                                                                      enc_embedding_size=encoding_embedding_size,\n",
    "                                                                      dec_embedding_size=decoding_embedding_size,\n",
    "                                                                      rnn_size=rnn_size,\n",
    "                                                                      num_layers=num_layers)\n",
    "\n",
    "    #Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_decoder_output[0].rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output[0].sample_id, name='predictions')\n",
    "        \n",
    "    ###loss###\n",
    "    with tf.name_scope('loss'):\n",
    "        mask = tf.sequence_mask(lengths=target_sequence_length,\n",
    "                                maxlen=max_target_sequence_length,\n",
    "                                dtype=tf.float32,\n",
    "                                name='mask')\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(logits=training_logits,\n",
    "                                                targets=targets,\n",
    "                                                weights=mask)\n",
    "    ###optimization###\n",
    "    with tf.name_scope('optimization'):\n",
    "        #optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        \n",
    "        #gradient clipping\n",
    "        grads_and_vars = optimizer.compute_gradients(cost)\n",
    "        clipped_grads = [(tf.clip_by_value(grad, -5, 5), var) for grad, var in grads_and_vars if grad is not None]\n",
    "        update = optimizer.apply_gradients(clipped_grads)\n",
    "    \n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    whole_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'EmbedSequence/embeddings:0' shape=(30, 15) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(65, 200) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0' shape=(100, 200) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'Variable:0' shape=(30, 15) dtype=float32_ref>, <tf.Variable 'decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(65, 200) dtype=float32_ref>, <tf.Variable 'decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel:0' shape=(100, 200) dtype=float32_ref>, <tf.Variable 'decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'decode/decoder/dense/kernel:0' shape=(50, 30) dtype=float32_ref>, <tf.Variable 'decode/decoder/dense/bias:0' shape=(30,) dtype=float32_ref>]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      targets, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_letter_to_int),\n",
    "                                                                      len(target_letter_to_int),\n",
    "                                                                      encoding_embedding_size, \n",
    "                                                                      decoding_embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    print(tf.trainable_variables())\n",
    "    \n",
    "print(len(source_letter_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    '''\n",
    "    Pad sentences with <PAD> so that each sentence of a batch has the same length.\n",
    "    '''\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    idx = 0\n",
    "    while (idx+batch_size) <= len(sources):\n",
    "        sources_batch = sources[idx:idx+batch_size]\n",
    "        targets_batch = targets[idx:idx+batch_size]\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        idx += batch_size\n",
    "        \n",
    "        targets_batch_lengths = []\n",
    "        for target in targets_batch:\n",
    "            targets_batch_lengths.append(len(target))\n",
    "        \n",
    "        sources_batch_lengths = []\n",
    "        for source in sources_batch:\n",
    "            sources_batch_lengths.append(len(source))\n",
    "            \n",
    "        yield pad_targets_batch, pad_sources_batch, targets_batch_lengths, sources_batch_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        # 注意，此處與課程投影片中的程式碼不同，須返回的長度是做padding前的長度\n",
    "        targets_batch_lengths = []\n",
    "        for target in targets_batch:\n",
    "            targets_batch_lengths.append(len(target))\n",
    "        \n",
    "        source_batch_lengths = []\n",
    "        for source in sources_batch:\n",
    "            source_batch_lengths.append(len(source))\n",
    "        \n",
    "        yield pad_targets_batch, pad_sources_batch, targets_batch_lengths, source_batch_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "We're now ready to train our model. If you run into OOM (out of memory) issues during training, try to decrease the batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/60 Batch: 20/77 - Loss:  3.124 - Validation loss:  3.113\n",
      "Epoch: 1/60 Batch: 40/77 - Loss:  2.975 - Validation loss:  2.965\n",
      "Epoch: 1/60 Batch: 60/77 - Loss:  2.706 - Validation loss:  2.710\n",
      "Epoch: 2/60 Batch: 20/77 - Loss:  2.331 - Validation loss:  2.356\n",
      "Epoch: 2/60 Batch: 40/77 - Loss:  2.251 - Validation loss:  2.259\n",
      "Epoch: 2/60 Batch: 60/77 - Loss:  2.126 - Validation loss:  2.121\n",
      "Epoch: 3/60 Batch: 20/77 - Loss:  1.960 - Validation loss:  1.944\n",
      "Epoch: 3/60 Batch: 40/77 - Loss:  1.906 - Validation loss:  1.885\n",
      "Epoch: 3/60 Batch: 60/77 - Loss:  1.821 - Validation loss:  1.817\n",
      "Epoch: 4/60 Batch: 20/77 - Loss:  1.694 - Validation loss:  1.712\n",
      "Epoch: 4/60 Batch: 40/77 - Loss:  1.637 - Validation loss:  1.663\n",
      "Epoch: 4/60 Batch: 60/77 - Loss:  1.557 - Validation loss:  1.605\n",
      "Epoch: 5/60 Batch: 20/77 - Loss:  1.461 - Validation loss:  1.500\n",
      "Epoch: 5/60 Batch: 40/77 - Loss:  1.422 - Validation loss:  1.444\n",
      "Epoch: 5/60 Batch: 60/77 - Loss:  1.338 - Validation loss:  1.379\n",
      "Epoch: 6/60 Batch: 20/77 - Loss:  1.247 - Validation loss:  1.276\n",
      "Epoch: 6/60 Batch: 40/77 - Loss:  1.208 - Validation loss:  1.235\n",
      "Epoch: 6/60 Batch: 60/77 - Loss:  1.150 - Validation loss:  1.170\n",
      "Epoch: 7/60 Batch: 20/77 - Loss:  1.074 - Validation loss:  1.087\n",
      "Epoch: 7/60 Batch: 40/77 - Loss:  1.045 - Validation loss:  1.052\n",
      "Epoch: 7/60 Batch: 60/77 - Loss:  0.996 - Validation loss:  0.998\n",
      "Epoch: 8/60 Batch: 20/77 - Loss:  0.917 - Validation loss:  0.923\n",
      "Epoch: 8/60 Batch: 40/77 - Loss:  0.899 - Validation loss:  0.888\n",
      "Epoch: 8/60 Batch: 60/77 - Loss:  0.855 - Validation loss:  0.847\n",
      "Epoch: 9/60 Batch: 20/77 - Loss:  0.785 - Validation loss:  0.785\n",
      "Epoch: 9/60 Batch: 40/77 - Loss:  0.779 - Validation loss:  0.754\n",
      "Epoch: 9/60 Batch: 60/77 - Loss:  0.744 - Validation loss:  0.721\n",
      "Epoch: 10/60 Batch: 20/77 - Loss:  0.678 - Validation loss:  0.672\n",
      "Epoch: 10/60 Batch: 40/77 - Loss:  0.677 - Validation loss:  0.656\n",
      "Epoch: 10/60 Batch: 60/77 - Loss:  0.646 - Validation loss:  0.623\n",
      "Epoch: 11/60 Batch: 20/77 - Loss:  0.589 - Validation loss:  0.579\n",
      "Epoch: 11/60 Batch: 40/77 - Loss:  0.601 - Validation loss:  0.570\n",
      "Epoch: 11/60 Batch: 60/77 - Loss:  0.568 - Validation loss:  0.542\n",
      "Epoch: 12/60 Batch: 20/77 - Loss:  0.520 - Validation loss:  0.505\n",
      "Epoch: 12/60 Batch: 40/77 - Loss:  0.545 - Validation loss:  0.497\n",
      "Epoch: 12/60 Batch: 60/77 - Loss:  0.508 - Validation loss:  0.477\n",
      "Epoch: 13/60 Batch: 20/77 - Loss:  0.457 - Validation loss:  0.446\n",
      "Epoch: 13/60 Batch: 40/77 - Loss:  0.479 - Validation loss:  0.434\n",
      "Epoch: 13/60 Batch: 60/77 - Loss:  0.447 - Validation loss:  0.421\n",
      "Epoch: 14/60 Batch: 20/77 - Loss:  0.402 - Validation loss:  0.390\n",
      "Epoch: 14/60 Batch: 40/77 - Loss:  0.420 - Validation loss:  0.378\n",
      "Epoch: 14/60 Batch: 60/77 - Loss:  0.394 - Validation loss:  0.369\n",
      "Epoch: 15/60 Batch: 20/77 - Loss:  0.351 - Validation loss:  0.337\n",
      "Epoch: 15/60 Batch: 40/77 - Loss:  0.366 - Validation loss:  0.330\n",
      "Epoch: 15/60 Batch: 60/77 - Loss:  0.343 - Validation loss:  0.322\n",
      "Epoch: 16/60 Batch: 20/77 - Loss:  0.307 - Validation loss:  0.290\n",
      "Epoch: 16/60 Batch: 40/77 - Loss:  0.317 - Validation loss:  0.284\n",
      "Epoch: 16/60 Batch: 60/77 - Loss:  0.297 - Validation loss:  0.280\n",
      "Epoch: 17/60 Batch: 20/77 - Loss:  0.266 - Validation loss:  0.252\n",
      "Epoch: 17/60 Batch: 40/77 - Loss:  0.277 - Validation loss:  0.246\n",
      "Epoch: 17/60 Batch: 60/77 - Loss:  0.257 - Validation loss:  0.241\n",
      "Epoch: 18/60 Batch: 20/77 - Loss:  0.225 - Validation loss:  0.219\n",
      "Epoch: 18/60 Batch: 40/77 - Loss:  0.230 - Validation loss:  0.214\n",
      "Epoch: 18/60 Batch: 60/77 - Loss:  0.216 - Validation loss:  0.206\n",
      "Epoch: 19/60 Batch: 20/77 - Loss:  0.188 - Validation loss:  0.191\n",
      "Epoch: 19/60 Batch: 40/77 - Loss:  0.192 - Validation loss:  0.183\n",
      "Epoch: 19/60 Batch: 60/77 - Loss:  0.182 - Validation loss:  0.175\n",
      "Epoch: 20/60 Batch: 20/77 - Loss:  0.157 - Validation loss:  0.165\n",
      "Epoch: 20/60 Batch: 40/77 - Loss:  0.163 - Validation loss:  0.157\n",
      "Epoch: 20/60 Batch: 60/77 - Loss:  0.156 - Validation loss:  0.152\n",
      "Epoch: 21/60 Batch: 20/77 - Loss:  0.133 - Validation loss:  0.138\n",
      "Epoch: 21/60 Batch: 40/77 - Loss:  0.139 - Validation loss:  0.137\n",
      "Epoch: 21/60 Batch: 60/77 - Loss:  0.131 - Validation loss:  0.131\n",
      "Epoch: 22/60 Batch: 20/77 - Loss:  0.114 - Validation loss:  0.120\n",
      "Epoch: 22/60 Batch: 40/77 - Loss:  0.121 - Validation loss:  0.124\n",
      "Epoch: 22/60 Batch: 60/77 - Loss:  0.112 - Validation loss:  0.114\n",
      "Epoch: 23/60 Batch: 20/77 - Loss:  0.098 - Validation loss:  0.107\n",
      "Epoch: 23/60 Batch: 40/77 - Loss:  0.105 - Validation loss:  0.112\n",
      "Epoch: 23/60 Batch: 60/77 - Loss:  0.098 - Validation loss:  0.101\n",
      "Epoch: 24/60 Batch: 20/77 - Loss:  0.083 - Validation loss:  0.097\n",
      "Epoch: 24/60 Batch: 40/77 - Loss:  0.091 - Validation loss:  0.098\n",
      "Epoch: 24/60 Batch: 60/77 - Loss:  0.085 - Validation loss:  0.093\n",
      "Epoch: 25/60 Batch: 20/77 - Loss:  0.071 - Validation loss:  0.089\n",
      "Epoch: 25/60 Batch: 40/77 - Loss:  0.079 - Validation loss:  0.087\n",
      "Epoch: 25/60 Batch: 60/77 - Loss:  0.073 - Validation loss:  0.084\n",
      "Epoch: 26/60 Batch: 20/77 - Loss:  0.061 - Validation loss:  0.082\n",
      "Epoch: 26/60 Batch: 40/77 - Loss:  0.070 - Validation loss:  0.079\n",
      "Epoch: 26/60 Batch: 60/77 - Loss:  0.063 - Validation loss:  0.075\n",
      "Epoch: 27/60 Batch: 20/77 - Loss:  0.054 - Validation loss:  0.075\n",
      "Epoch: 27/60 Batch: 40/77 - Loss:  0.061 - Validation loss:  0.072\n",
      "Epoch: 27/60 Batch: 60/77 - Loss:  0.056 - Validation loss:  0.068\n",
      "Epoch: 28/60 Batch: 20/77 - Loss:  0.048 - Validation loss:  0.069\n",
      "Epoch: 28/60 Batch: 40/77 - Loss:  0.055 - Validation loss:  0.067\n",
      "Epoch: 28/60 Batch: 60/77 - Loss:  0.050 - Validation loss:  0.060\n",
      "Epoch: 29/60 Batch: 20/77 - Loss:  0.043 - Validation loss:  0.063\n",
      "Epoch: 29/60 Batch: 40/77 - Loss:  0.049 - Validation loss:  0.062\n",
      "Epoch: 29/60 Batch: 60/77 - Loss:  0.046 - Validation loss:  0.053\n",
      "Epoch: 30/60 Batch: 20/77 - Loss:  0.038 - Validation loss:  0.059\n",
      "Epoch: 30/60 Batch: 40/77 - Loss:  0.044 - Validation loss:  0.058\n",
      "Epoch: 30/60 Batch: 60/77 - Loss:  0.042 - Validation loss:  0.049\n",
      "Epoch: 31/60 Batch: 20/77 - Loss:  0.034 - Validation loss:  0.052\n",
      "Epoch: 31/60 Batch: 40/77 - Loss:  0.039 - Validation loss:  0.055\n",
      "Epoch: 31/60 Batch: 60/77 - Loss:  0.037 - Validation loss:  0.045\n",
      "Epoch: 32/60 Batch: 20/77 - Loss:  0.032 - Validation loss:  0.048\n",
      "Epoch: 32/60 Batch: 40/77 - Loss:  0.036 - Validation loss:  0.049\n",
      "Epoch: 32/60 Batch: 60/77 - Loss:  0.034 - Validation loss:  0.041\n",
      "Epoch: 33/60 Batch: 20/77 - Loss:  0.029 - Validation loss:  0.045\n",
      "Epoch: 33/60 Batch: 40/77 - Loss:  0.034 - Validation loss:  0.044\n",
      "Epoch: 33/60 Batch: 60/77 - Loss:  0.032 - Validation loss:  0.040\n",
      "Epoch: 34/60 Batch: 20/77 - Loss:  0.025 - Validation loss:  0.041\n",
      "Epoch: 34/60 Batch: 40/77 - Loss:  0.031 - Validation loss:  0.041\n",
      "Epoch: 34/60 Batch: 60/77 - Loss:  0.027 - Validation loss:  0.037\n",
      "Epoch: 35/60 Batch: 20/77 - Loss:  0.023 - Validation loss:  0.036\n",
      "Epoch: 35/60 Batch: 40/77 - Loss:  0.027 - Validation loss:  0.039\n",
      "Epoch: 35/60 Batch: 60/77 - Loss:  0.024 - Validation loss:  0.032\n",
      "Epoch: 36/60 Batch: 20/77 - Loss:  0.021 - Validation loss:  0.031\n",
      "Epoch: 36/60 Batch: 40/77 - Loss:  0.024 - Validation loss:  0.035\n",
      "Epoch: 36/60 Batch: 60/77 - Loss:  0.022 - Validation loss:  0.030\n",
      "Epoch: 37/60 Batch: 20/77 - Loss:  0.020 - Validation loss:  0.029\n",
      "Epoch: 37/60 Batch: 40/77 - Loss:  0.023 - Validation loss:  0.033\n",
      "Epoch: 37/60 Batch: 60/77 - Loss:  0.021 - Validation loss:  0.028\n",
      "Epoch: 38/60 Batch: 20/77 - Loss:  0.018 - Validation loss:  0.027\n",
      "Epoch: 38/60 Batch: 40/77 - Loss:  0.021 - Validation loss:  0.031\n",
      "Epoch: 38/60 Batch: 60/77 - Loss:  0.019 - Validation loss:  0.026\n",
      "Epoch: 39/60 Batch: 20/77 - Loss:  0.017 - Validation loss:  0.025\n",
      "Epoch: 39/60 Batch: 40/77 - Loss:  0.019 - Validation loss:  0.028\n",
      "Epoch: 39/60 Batch: 60/77 - Loss:  0.017 - Validation loss:  0.024\n",
      "Epoch: 40/60 Batch: 20/77 - Loss:  0.016 - Validation loss:  0.024\n",
      "Epoch: 40/60 Batch: 40/77 - Loss:  0.017 - Validation loss:  0.026\n",
      "Epoch: 40/60 Batch: 60/77 - Loss:  0.016 - Validation loss:  0.022\n",
      "Epoch: 41/60 Batch: 20/77 - Loss:  0.015 - Validation loss:  0.024\n",
      "Epoch: 41/60 Batch: 40/77 - Loss:  0.016 - Validation loss:  0.024\n",
      "Epoch: 41/60 Batch: 60/77 - Loss:  0.015 - Validation loss:  0.020\n",
      "Epoch: 42/60 Batch: 20/77 - Loss:  0.014 - Validation loss:  0.023\n",
      "Epoch: 42/60 Batch: 40/77 - Loss:  0.015 - Validation loss:  0.021\n",
      "Epoch: 42/60 Batch: 60/77 - Loss:  0.014 - Validation loss:  0.018\n",
      "Epoch: 43/60 Batch: 20/77 - Loss:  0.013 - Validation loss:  0.021\n",
      "Epoch: 43/60 Batch: 40/77 - Loss:  0.013 - Validation loss:  0.020\n",
      "Epoch: 43/60 Batch: 60/77 - Loss:  0.013 - Validation loss:  0.017\n",
      "Epoch: 44/60 Batch: 20/77 - Loss:  0.012 - Validation loss:  0.020\n",
      "Epoch: 44/60 Batch: 40/77 - Loss:  0.012 - Validation loss:  0.018\n",
      "Epoch: 44/60 Batch: 60/77 - Loss:  0.012 - Validation loss:  0.015\n",
      "Epoch: 45/60 Batch: 20/77 - Loss:  0.011 - Validation loss:  0.020\n",
      "Epoch: 45/60 Batch: 40/77 - Loss:  0.011 - Validation loss:  0.017\n",
      "Epoch: 45/60 Batch: 60/77 - Loss:  0.012 - Validation loss:  0.014\n",
      "Epoch: 46/60 Batch: 20/77 - Loss:  0.010 - Validation loss:  0.020\n",
      "Epoch: 46/60 Batch: 40/77 - Loss:  0.011 - Validation loss:  0.016\n",
      "Epoch: 46/60 Batch: 60/77 - Loss:  0.011 - Validation loss:  0.014\n",
      "Epoch: 47/60 Batch: 20/77 - Loss:  0.010 - Validation loss:  0.019\n",
      "Epoch: 47/60 Batch: 40/77 - Loss:  0.010 - Validation loss:  0.016\n",
      "Epoch: 47/60 Batch: 60/77 - Loss:  0.010 - Validation loss:  0.014\n",
      "Epoch: 48/60 Batch: 20/77 - Loss:  0.009 - Validation loss:  0.018\n",
      "Epoch: 48/60 Batch: 40/77 - Loss:  0.009 - Validation loss:  0.015\n",
      "Epoch: 48/60 Batch: 60/77 - Loss:  0.010 - Validation loss:  0.014\n",
      "Epoch: 49/60 Batch: 20/77 - Loss:  0.008 - Validation loss:  0.017\n",
      "Epoch: 49/60 Batch: 40/77 - Loss:  0.009 - Validation loss:  0.016\n",
      "Epoch: 49/60 Batch: 60/77 - Loss:  0.009 - Validation loss:  0.013\n",
      "Epoch: 50/60 Batch: 20/77 - Loss:  0.008 - Validation loss:  0.015\n",
      "Epoch: 50/60 Batch: 40/77 - Loss:  0.008 - Validation loss:  0.016\n",
      "Epoch: 50/60 Batch: 60/77 - Loss:  0.008 - Validation loss:  0.012\n",
      "Epoch: 51/60 Batch: 20/77 - Loss:  0.007 - Validation loss:  0.013\n",
      "Epoch: 51/60 Batch: 40/77 - Loss:  0.008 - Validation loss:  0.015\n",
      "Epoch: 51/60 Batch: 60/77 - Loss:  0.007 - Validation loss:  0.011\n",
      "Epoch: 52/60 Batch: 20/77 - Loss:  0.007 - Validation loss:  0.012\n",
      "Epoch: 52/60 Batch: 40/77 - Loss:  0.007 - Validation loss:  0.015\n",
      "Epoch: 52/60 Batch: 60/77 - Loss:  0.007 - Validation loss:  0.011\n",
      "Epoch: 53/60 Batch: 20/77 - Loss:  0.007 - Validation loss:  0.012\n",
      "Epoch: 53/60 Batch: 40/77 - Loss:  0.007 - Validation loss:  0.013\n",
      "Epoch: 53/60 Batch: 60/77 - Loss:  0.006 - Validation loss:  0.010\n",
      "Epoch: 54/60 Batch: 20/77 - Loss:  0.006 - Validation loss:  0.012\n",
      "Epoch: 54/60 Batch: 40/77 - Loss:  0.006 - Validation loss:  0.012\n",
      "Epoch: 54/60 Batch: 60/77 - Loss:  0.006 - Validation loss:  0.010\n",
      "Epoch: 55/60 Batch: 20/77 - Loss:  0.006 - Validation loss:  0.011\n",
      "Epoch: 55/60 Batch: 40/77 - Loss:  0.006 - Validation loss:  0.011\n",
      "Epoch: 55/60 Batch: 60/77 - Loss:  0.006 - Validation loss:  0.009\n",
      "Epoch: 56/60 Batch: 20/77 - Loss:  0.006 - Validation loss:  0.011\n",
      "Epoch: 56/60 Batch: 40/77 - Loss:  0.005 - Validation loss:  0.010\n",
      "Epoch: 56/60 Batch: 60/77 - Loss:  0.005 - Validation loss:  0.009\n",
      "Epoch: 57/60 Batch: 20/77 - Loss:  0.005 - Validation loss:  0.010\n",
      "Epoch: 57/60 Batch: 40/77 - Loss:  0.005 - Validation loss:  0.010\n",
      "Epoch: 57/60 Batch: 60/77 - Loss:  0.005 - Validation loss:  0.008\n",
      "Epoch: 58/60 Batch: 20/77 - Loss:  0.005 - Validation loss:  0.010\n",
      "Epoch: 58/60 Batch: 40/77 - Loss:  0.005 - Validation loss:  0.009\n",
      "Epoch: 58/60 Batch: 60/77 - Loss:  0.005 - Validation loss:  0.008\n",
      "Epoch: 59/60 Batch: 20/77 - Loss:  0.005 - Validation loss:  0.009\n",
      "Epoch: 59/60 Batch: 40/77 - Loss:  0.005 - Validation loss:  0.008\n",
      "Epoch: 59/60 Batch: 60/77 - Loss:  0.005 - Validation loss:  0.007\n",
      "Epoch: 60/60 Batch: 20/77 - Loss:  0.004 - Validation loss:  0.009\n",
      "Epoch: 60/60 Batch: 40/77 - Loss:  0.004 - Validation loss:  0.008\n",
      "Epoch: 60/60 Batch: 60/77 - Loss:  0.004 - Validation loss:  0.007\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "#Split data to training and validation sets\n",
    "train_source = source_letter_ids[batch_size:]\n",
    "train_target = target_letter_ids[batch_size:]\n",
    "valid_source = source_letter_ids[:batch_size]\n",
    "valid_target = target_letter_ids[:batch_size]\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target,\n",
    "                                                                                                            valid_source,\n",
    "                                                                                                            batch_size,\n",
    "                                                                                                            source_letter_to_int['<PAD>'],\n",
    "                                                                                                            target_letter_to_int['<PAD>']))\n",
    "display_step = 20\n",
    "\n",
    "checkpoint = \"model/best_model_20190131.ckpt\"\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epc in range(1, epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(get_batches(train_target,\n",
    "                                                                                                              train_source,\n",
    "                                                                                                              batch_size,\n",
    "                                                                                                              source_letter_to_int['<PAD>'],\n",
    "                                                                                                              target_letter_to_int['<PAD>'])):\n",
    "            _, loss, train_out, inference_out = sess.run([update, cost, training_decoder_output, inference_decoder_output],\n",
    "                                                          feed_dict={input_data: sources_batch,\n",
    "                                                                     targets: targets_batch,\n",
    "                                                                     lr: learning_rate,\n",
    "                                                                     target_sequence_length: targets_lengths,\n",
    "                                                                     source_sequence_length: sources_lengths})\n",
    "            \n",
    "            \n",
    "            \n",
    "            if batch_i % display_step == 0 and batch_i >0:\n",
    "                \n",
    "                # calculate validation cost\n",
    "                validation_loss = sess.run(cost, feed_dict={input_data: valid_sources_batch,\n",
    "                                                              targets: valid_targets_batch,\n",
    "                                                              lr: learning_rate,\n",
    "                                                              target_sequence_length: valid_targets_lengths,\n",
    "                                                              source_sequence_length: valid_sources_lengths})\n",
    "                \n",
    "                print('Epoch: {}/{} Batch: {}/{} - Loss: {:>6.3f} - Validation loss: {:>6.3f}'.format(epc,\n",
    "                                                                                                      epochs,\n",
    "                                                                                                      batch_i,\n",
    "                                                                                                      len(train_source)//batch_size,\n",
    "                                                                                                      loss,\n",
    "                                                                                                      validation_loss))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Output of Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 50), (128, 50))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final state\n",
    "train_out[1][0].h.shape, train_out[1][0].c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 8, 7, 6, 3, 4, 7, 6, 4, 3, 8, 3, 4, 3, 4, 8, 7, 6, 3, 7, 6,\n",
       "       4, 6, 2, 8, 6, 6, 3, 7, 6, 2, 3, 3, 3, 5, 5, 6, 4, 6, 3, 5, 3, 5,\n",
       "       8, 6, 8, 7, 8, 8, 6, 8, 7, 8, 7, 8, 7, 6, 6, 4, 4, 7, 2, 8, 7, 2,\n",
       "       2, 3, 2, 8, 2, 3, 3, 2, 2, 4, 3, 8, 4, 4, 7, 8, 6, 7, 2, 4, 2, 6,\n",
       "       7, 7, 7, 8, 3, 3, 5, 4, 3, 3, 8, 6, 4, 8, 5, 3, 6, 2, 2, 4, 6, 4,\n",
       "       7, 8, 7, 7, 3, 7, 6, 8, 4, 8, 6, 5, 6, 7, 8, 2, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final sequence length\n",
    "train_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_letter_to_int), len(target_letter_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 8, 30), (128, 8))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final outputs = (rnn_output, sample_id)\n",
    "train_out[0].rnn_output.shape, train_out[0].sample_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first letter of the first sequence is 12\n",
    "np.argmax(train_out[0].rnn_output[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  6, 28, ..., 17,  3,  0],\n",
       "       [29, 24, 13, ..., 26, 25,  3],\n",
       "       [28,  8, 18, ..., 25, 25,  3],\n",
       "       ...,\n",
       "       [17,  3,  0, ...,  0,  0,  0],\n",
       "       [ 9, 23,  3, ...,  0,  0,  0],\n",
       "       [24, 18,  3, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the maximum sequence length of this last batch is 8\n",
    "train_out[0].sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 8, 30), (128, 8))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_out[0].rnn_output.shape, inference_out[0].sample_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def source_to_seq(text):\n",
    "    '''\n",
    "    Prepare the text for the model\n",
    "    '''\n",
    "    sequence_length = 7\n",
    "    return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in text] + [source_letter_to_int['<PAD>']]*(sequence_length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def source_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    sequence_length = 7\n",
    "    return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in text]+ [source_letter_to_int['<PAD>']]*(sequence_length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/best_model_20190131.ckpt\n",
      "Original Text: edcba\n",
      "\n",
      "Source\n",
      "Word Ids: [4, 12, 10, 15, 29, 0, 0]\n",
      "Input Words: e d c b a <PAD> <PAD>\n",
      "\n",
      "Target\n",
      "Word Ids: [29, 15, 10, 12, 4]\n",
      "Response Words: a b c d e\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'edcba'\n",
    "text = source_to_seq(input_sentence)\n",
    "\n",
    "checkpoint = \"model/best_model_20190131.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    ###Load saved model###\n",
    "    loader = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    \n",
    "    input_data = loaded_graph.get_tensor_by_name('input/input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('input/source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('input/target_sequence_length:0')\n",
    "    \n",
    "    ###Multiply by batch_size to match the model's input parameters###\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size,\n",
    "                                      target_sequence_length: [len(input_sentence)]*batch_size,\n",
    "                                      source_sequence_length: [len(input_sentence)]*batch_size})[0]\n",
    "\n",
    "pad = source_letter_to_int['<PAD>']\n",
    "print('Original Text:', input_sentence)\n",
    "print('\\nSource')\n",
    "print('Word Ids: {}'.format([i for i in text]))\n",
    "print('Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "print('\\nTarget')\n",
    "print('Word Ids: {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/best_model.ckpt\n",
      "Original Text: lexa\n",
      "\n",
      "Source\n",
      "  Word Ids:    [16, 4, 27, 18, 0, 0, 0]\n",
      "  Input Words: l e x a <PAD> <PAD> <PAD>\n",
      "\n",
      "Target\n",
      "  Word Ids:       [18, 4, 16, 27]\n",
      "  Response Words: a e l x\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'lexa'\n",
    "text = source_to_seq(input_sentence)\n",
    "\n",
    "checkpoint = \"model/best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      target_sequence_length: [len(input_sentence)]*batch_size, \n",
    "                                      source_sequence_length: [len(input_sentence)]*batch_size})[0]\n",
    "\n",
    "\n",
    "pad = source_letter_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nSource')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "print('\\nTarget')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
